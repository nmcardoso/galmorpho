{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "splus_model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Dfhn_xuMzlGv"
      ],
      "mount_file_id": "1AZ581kRn-dsuPaP-9xt3ys9A5ZCXVaA1",
      "authorship_tag": "ABX9TyN6+di4PkLasrFSICEgTL0w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nmcardoso/splus-model/blob/master/splus_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vivABYTN239V",
        "colab_type": "code",
        "outputId": "d34825b9-4504-4b64-c06b-a6e6b4dd0270",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "source": [
        "%tensorflow_version 1.x \n",
        "import random\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "import os\n",
        "import glob\n",
        "import multiprocessing as mp\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras import utils\n",
        "from keras import applications\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras.applications import VGG16\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from progressbar import progressbar\n",
        "\n",
        "!pip3 install git+https://github.com/nmcardoso/fitsbook-python\n",
        "import fitsbook as fb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/nmcardoso/fitsbook-python\n",
            "  Cloning https://github.com/nmcardoso/fitsbook-python to /tmp/pip-req-build-j9_1w0yk\n",
            "  Running command git clone -q https://github.com/nmcardoso/fitsbook-python /tmp/pip-req-build-j9_1w0yk\n",
            "Requirement already satisfied (use --upgrade to upgrade): fitsbook==0.1 from git+https://github.com/nmcardoso/fitsbook-python in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fitsbook==0.1) (2.21.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from fitsbook==0.1) (2.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fitsbook==0.1) (1.18.3)\n",
            "Requirement already satisfied: tensorflow in /tensorflow-1.15.2/python3.6 (from fitsbook==0.1) (1.15.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fitsbook==0.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fitsbook==0.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fitsbook==0.1) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fitsbook==0.1) (2.8)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->fitsbook==0.1) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->fitsbook==0.1) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->fitsbook==0.1) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->fitsbook==0.1) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->fitsbook==0.1) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->fitsbook==0.1) (3.13)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->fitsbook==0.1) (3.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->fitsbook==0.1) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->fitsbook==0.1) (1.28.1)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /tensorflow-1.15.2/python3.6 (from tensorflow->fitsbook==0.1) (1.15.1)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /tensorflow-1.15.2/python3.6 (from tensorflow->fitsbook==0.1) (1.15.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->fitsbook==0.1) (0.2.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->fitsbook==0.1) (3.2.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->fitsbook==0.1) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow->fitsbook==0.1) (0.34.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->fitsbook==0.1) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->fitsbook==0.1) (0.9.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->fitsbook==0.1) (0.8.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow->fitsbook==0.1) (46.1.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->fitsbook==0.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->fitsbook==0.1) (3.2.1)\n",
            "Building wheels for collected packages: fitsbook\n",
            "  Building wheel for fitsbook (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fitsbook: filename=fitsbook-0.1-cp36-none-any.whl size=2181 sha256=b231b08981b593448a2979fd25a3036d9acd18e04429e88ba2a10c403c825db9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-daesvrtb/wheels/99/2a/a0/b37947be424acac83a373b37b38a2bfaa4557b049c19426655\n",
            "Successfully built fitsbook\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqLHp_vx9e2V",
        "colab_type": "code",
        "outputId": "04c4d745-fb79-43c9-f9b6-afb506adcdd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Reprodutibilidade\n",
        "# https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
        "\n",
        "os.environ['PYTHONHASHSEED'] = '12'\n",
        "np.random.seed(123)\n",
        "random.seed(1234)\n",
        "tf.set_random_seed(12345)\n",
        "\n",
        "# session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, \n",
        "#                               inter_op_parallelism_threads=1)\n",
        "# sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "# K.set_session(sess)\n",
        "\n",
        "print('Sementes Plantadas!')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sementes Plantadas!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8IDVH7Pz1so",
        "colab_type": "text"
      },
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVf7HjPhJYsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip /content/drive/'My Drive'/datasets/stamps_scaled.zip -d dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2cNynjnqAWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp \"/content/drive/My Drive/datasets/test/stamps_scaled_12ch_subset.tar.xz\" dataset.tar.xz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkaeAapaNCyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -C dataset -xvf \"/content/drive/My Drive/datasets/test/stamps_scaled_12ch_subset.tar.xz\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faZlo4FERwdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/drive/'My Drive'/datasets/stamps_scaled_12ch_subset.csv /content/dataset.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbYRpllzlv7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -C dataset -xvf dataset.tar.xz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKouK0TO1odD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NpySequence(utils.Sequence):\n",
        "  \"\"\"Generate data for Keras\"\"\"\n",
        "  def __init__(self, files, labels, classes, image_size=(256, 256), channels=3, batch_size=32):\n",
        "    self.files = files\n",
        "    self.labels = self.__to_categorical(labels, classes)\n",
        "    self.image_size = image_size\n",
        "    self.channels = channels\n",
        "    self.batch_size = batch_size\n",
        "    self.indexes = [i for i in range(len(files))]\n",
        "    np.random.shuffle(self.indexes)\n",
        "    # self.on_epoch_end()\n",
        "  \n",
        "  def __len__(self):\n",
        "    \"\"\"The number of batches per epoch\"\"\"\n",
        "    return int(np.floor(len(self.files) / self.batch_size))\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    \"\"\"Generate one batch of data\"\"\"\n",
        "    partition = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
        "    X, Y = self.__data_generation(partition)\n",
        "    return X, Y\n",
        "  \n",
        "  def on_epoch_end(self):\n",
        "    \"\"\"Shuffle files list and remap labels\"\"\"\n",
        "    np.random.shuffle(self.indexes)\n",
        "    \n",
        "  def __to_categorical(self, labels, classes):\n",
        "    \"\"\"Labels to numbers\"\"\"\n",
        "    classes_dict = {c:i for i,c in enumerate(classes)}\n",
        "    return [classes_dict[l] for l in labels]\n",
        "\n",
        "  def __data_generation(self, partition):\n",
        "    \"\"\"Load npy file and associate with a label\"\"\"\n",
        "    X = np.empty((self.batch_size, *self.image_size, self.channels))\n",
        "    Y = np.empty((self.batch_size,), dtype=int)\n",
        "\n",
        "    for i, index in enumerate(partition):\n",
        "      X[i,] = np.load(self.files[index])\n",
        "      Y[i] = self.labels[index]\n",
        "\n",
        "    return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ4ZuEFRzlJG",
        "colab_type": "text"
      },
      "source": [
        "## Modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dfhn_xuMzlGv",
        "colab_type": "text"
      },
      "source": [
        "### NÃ£o usados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjJXg_UMzsSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inception_module(layer_in, f1, f2_in, f2_out, f3_in, f3_out, f4_out):\n",
        "  # 1x1 conv\n",
        "  conv1 = layers.Conv2D(f1, (1, 1), padding='same', activation='relu')(layer_in)\n",
        "\n",
        "  # 3x3 conv\n",
        "  conv3 = layers.Conv2D(f2_in, (1, 1), padding='same', activation='relu')(layer_in)\n",
        "  conv3 = layers.Conv2D(f2_out, (3, 3), padding='same', activation='relu')(layer_in)\n",
        "\n",
        "  # 5x5 conv\n",
        "  conv5 = layers.Conv2D(f3_in, (1, 1), padding='same', activation='relu')(layer_in)\n",
        "  conv5 = layers.Conv2D(f3_out, (5, 5), padding='same', activation='relu')(conv5)\n",
        "\n",
        "  # 3x3 max pooling\n",
        "  pool = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same')(layer_in)\n",
        "  pool = layers.Conv2D(f4_out, (1, 1), padding='same', activation='relu')(pool)\n",
        "\n",
        "  # concatenate filters, assumes filters/channels last\n",
        "  layer_out = layers.concatenate([conv1, conv3, conv5, pool], axis=-1)\n",
        "  return layer_out\n",
        "\n",
        "def compile_inception_model(image_size):\n",
        "  input_layer = layers.Input(shape=image_size + (3,))\n",
        "  conv1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
        "  pool1 = layers.MaxPool2D((2, 2))(conv1)\n",
        "  conv2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
        "  pool2 = layers.MaxPool2D((2, 2))(conv2)\n",
        "  iblock1 = inception_module(input_layer, 32, 48, 64, 8, 16, 16)\n",
        "  iblock2 = inception_module(iblock1, 64, 48, 96, 16, 48, 32)\n",
        "  flatten = layers.Flatten()(iblock2)\n",
        "  dense1 = layers.Dense(1024, activation='relu')(flatten)\n",
        "  dropout1 = layers.Dropout(.3)(dense1)\n",
        "  dense2 = layers.Dense(512, activation='relu')(dropout1)\n",
        "  dropout2 = layers.Dropout(.3)(dense2)\n",
        "  dense3 = layers.Dense(256, activation='relu')(dropout2)\n",
        "  dropout3 = layers.Dropout(.3)(dense3)\n",
        "  output = layers.Dense(1, activation='sigmoid')(dropout3)\n",
        "  model = models.Model(inputs=input_layer, outputs=output)\n",
        "  model.name = 'splus_inception'\n",
        "  utils.plot_model(model, to_file='model.png')\n",
        "  model.compile(optimizer=optimizers.RMSprop(lr=5e-5),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['acc'])\n",
        "  return model\n",
        "\n",
        "def compile_inception_pretrained(image_size):\n",
        "  base_model = applications.InceptionV3(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
        "\n",
        "  x = base_model.output\n",
        "  x = layers.GlobalAveragePooling2D()(x)\n",
        "  x = layers.Dense(1024, activation='relu')(x)\n",
        "  output = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "  model = models.Model(inputs=base_model.input, outputs=output)\n",
        "  model.name = 'splus_inception_pretrained'\n",
        "\n",
        "  for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "  \n",
        "  model.compile(optimizer=optimizers.RMSprop(lr=5e-5),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['acc'])\n",
        "  \n",
        "  return model\n",
        "\n",
        "def compile_VGG16(image_size):\n",
        "  model = models.Sequential()\n",
        "  model.name = 'splus_vgg16_impl'\n",
        "  model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=(image_size + (3,))))\n",
        "  model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(2048, activation='relu'))\n",
        "  model.add(layers.Dropout(.5))\n",
        "  model.add(layers.Dense(1024, activation='relu'))\n",
        "  model.add(layers.Dropout(.5))\n",
        "  model.add(layers.Dense(512, activation='relu'))\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['acc'])\n",
        "  return model\n",
        "\n",
        "def compile_multidim_model(image_size):\n",
        "  input_tensor = layers.Input(shape=image_size + (1,))\n",
        "\n",
        "  branch_outputs = []\n",
        "  for i in range(3):\n",
        "    out = layers.Lambda(lambda x: x[:, i])(input_tensor)\n",
        "    out = layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(image_size + (1,)))(out)\n",
        "    out = layers.MaxPooling2D((2, 2))(out)\n",
        "    out = layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(image_size + (1,)))(out)\n",
        "    out = layers.MaxPooling2D((2, 2))(out)\n",
        "    out = layers.Conv2D(128, (3, 3), activation='relu', padding='same', input_shape=(image_size + (1,)))(out)\n",
        "    out = layers.MaxPooling2D((2, 2))(out)\n",
        "    branch_outputs.append(out)\n",
        "  \n",
        "  x = layers.Concatenate()(branch_outputs)\n",
        "  x = layers.Flatten()(x)\n",
        "  x = layers.Dense(1024, activation='relu')(x)\n",
        "  x = layers.Dropout(.5)(x)\n",
        "  x = layers.Dense(512, activation='relu')(x)\n",
        "  x = layers.Dropout(.5)(x)\n",
        "  x = layers.Dense(256, activation='relu')(x)\n",
        "  x = layers.Dropout(.5)(x)\n",
        "  x = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "  model = models.Model(inputs=input_tensor, outputs=x)\n",
        "  model.name = 'splus_multidim_conv'\n",
        "  model.compile(optimizer=optimizers.RMSprop(lr=5e-5),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['acc'])\n",
        "  utils.plot_model(model, to_file='model.png')\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni3VnVz_zvzw",
        "colab_type": "text"
      },
      "source": [
        "### Usados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ghBzSbF9fpw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess():\n",
        "  global train_generator, test_generator, val_generator\n",
        "  df = pd.read_csv('/content/dataset.csv')\n",
        "  E = df[df.CLASS == 'E']\n",
        "  S = df[df.CLASS == 'S']\n",
        "\n",
        "  # train_df = pd.concat([E[0:3127], S[0:951]])\n",
        "  train_df = pd.concat([E[0:1400], S[0:1000]])\n",
        "  train_X = [f'/content/dataset/{f}.npy' for f in train_df.ID.tolist()]\n",
        "  train_Y = train_df.CLASS.tolist()\n",
        "\n",
        "  # val_df = pd.concat([E[3128:5004], S[952:1522]])\n",
        "  val_df = pd.concat([E[1401:2100], S[1001:1500]])\n",
        "  val_X = [f'/content/dataset/{f}.npy' for f in val_df.ID.tolist()]\n",
        "  val_Y = val_df.CLASS.tolist()\n",
        "\n",
        "  # test_df = pd.concat([E[5005:], S[1523:]])\n",
        "  test_df = pd.concat([E[2101:], S[1501:]])\n",
        "  test_X = [f'/content/dataset/{f}.npy' for f in test_df.ID.tolist()]\n",
        "  test_Y = test_df.CLASS.tolist()\n",
        "\n",
        "  train_generator = NpySequence(train_X, train_Y, ['E', 'S'], channels=12, batch_size=20)\n",
        "  val_generator = NpySequence(val_X, val_Y, ['E', 'S'], channels=12, batch_size=20)\n",
        "  test_generator = NpySequence(test_X, test_Y, ['E', 'S'], channels=12, batch_size=20)\n",
        "\n",
        "def compile_model(image_size):\n",
        "  model = models.Sequential()\n",
        "  model.name = 'splus_convolutional'\n",
        "  model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(image_size + (12,))))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(1024, activation='relu'))\n",
        "  model.add(layers.Dropout(.5))\n",
        "  model.add(layers.Dense(512, activation='relu'))\n",
        "  model.add(layers.Dropout(.5))\n",
        "  model.add(layers.Dense(256, activation='relu'))\n",
        "  model.add(layers.Dropout(.5))\n",
        "  model.add(layers.Dense(128, activation='relu'))\n",
        "  model.add(layers.Dropout(.5))\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dropout(.5))\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(optimizer=optimizers.RMSprop(lr=5e-5),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['acc'])\n",
        "  return model\n",
        "\n",
        "def fit(model, epochs=1):\n",
        "  global history, train_generator, val_generator\n",
        "  history = model.fit_generator(\n",
        "      train_generator,\n",
        "      # steps_per_epoch=80,\n",
        "      # validation_steps=30,\n",
        "      epochs=epochs,\n",
        "      validation_data=val_generator,\n",
        "      callbacks=[fb.callbacks.FitsbookCallback()],\n",
        "      use_multiprocessing=True,\n",
        "      workers=2\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm3MY-kw29F0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocess()\n",
        "m = compile_model((256, 256))\n",
        "fit(m, 300)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}