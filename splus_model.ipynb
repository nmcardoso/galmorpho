{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "splus_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1AZ581kRn-dsuPaP-9xt3ys9A5ZCXVaA1",
      "authorship_tag": "ABX9TyOkCeQBDvwfFl4UFzvZqp+b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nmcardoso/splus-model/blob/master/splus_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vivABYTN239V",
        "colab_type": "code",
        "outputId": "a28afbc6-35a5-4f46-b91f-d3219fd613d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "%tensorflow_version 1.x \n",
        "import random\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "import os\n",
        "import glob\n",
        "import multiprocessing as mp\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras import utils\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras.applications import VGG16\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from progressbar import progressbar\n",
        "\n",
        "!pip3 install git+https://github.com/nmcardoso/fitsbook-python\n",
        "import fitsbook as fb"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/nmcardoso/fitsbook-python\n",
            "  Cloning https://github.com/nmcardoso/fitsbook-python to /tmp/pip-req-build-be38z8e1\n",
            "  Running command git clone -q https://github.com/nmcardoso/fitsbook-python /tmp/pip-req-build-be38z8e1\n",
            "Requirement already satisfied (use --upgrade to upgrade): fitsbook==0.1 from git+https://github.com/nmcardoso/fitsbook-python in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fitsbook==0.1) (2.21.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from fitsbook==0.1) (2.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fitsbook==0.1) (1.18.2)\n",
            "Requirement already satisfied: tensorflow in /tensorflow-1.15.2/python3.6 (from fitsbook==0.1) (1.15.2)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fitsbook==0.1) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fitsbook==0.1) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fitsbook==0.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fitsbook==0.1) (2019.11.28)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras->fitsbook==0.1) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->fitsbook==0.1) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras->fitsbook==0.1) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->fitsbook==0.1) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->fitsbook==0.1) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->fitsbook==0.1) (2.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->fitsbook==0.1) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->fitsbook==0.1) (3.10.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /tensorflow-1.15.2/python3.6 (from tensorflow->fitsbook==0.1) (1.15.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->fitsbook==0.1) (0.2.2)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /tensorflow-1.15.2/python3.6 (from tensorflow->fitsbook==0.1) (1.15.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->fitsbook==0.1) (1.27.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->fitsbook==0.1) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->fitsbook==0.1) (0.9.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow->fitsbook==0.1) (0.34.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->fitsbook==0.1) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->fitsbook==0.1) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->fitsbook==0.1) (3.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow->fitsbook==0.1) (46.1.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->fitsbook==0.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->fitsbook==0.1) (3.2.1)\n",
            "Building wheels for collected packages: fitsbook\n",
            "  Building wheel for fitsbook (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fitsbook: filename=fitsbook-0.1-cp36-none-any.whl size=2181 sha256=94bb27182ca4e37eb8e209846cd11c7db95a886b9474b14645c51654b5f5df46\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3ynbycdl/wheels/99/2a/a0/b37947be424acac83a373b37b38a2bfaa4557b049c19426655\n",
            "Successfully built fitsbook\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqLHp_vx9e2V",
        "colab_type": "code",
        "outputId": "5217829e-574e-4689-e8d6-661e689df64c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Reprodutibilidade\n",
        "# https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
        "\n",
        "os.environ['PYTHONHASHSEED'] = '12'\n",
        "np.random.seed(123)\n",
        "random.seed(1234)\n",
        "tf.set_random_seed(12345)\n",
        "\n",
        "# session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, \n",
        "#                               inter_op_parallelism_threads=1)\n",
        "# sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "# K.set_session(sess)\n",
        "\n",
        "print('Sementes Plantadas!')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sementes Plantadas!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVf7HjPhJYsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip /content/drive/'My Drive'/datasets/stamps_scaled.zip -d dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkaeAapaNCyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -zxvf /content/drive/'My Drive'/datasets/dataset.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faZlo4FERwdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/drive/'My Drive'/datasets/stamps_scaled.csv /content/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKouK0TO1odD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NpySequence(utils.Sequence):\n",
        "  \"\"\"Generate data for Keras\"\"\"\n",
        "  def __init__(self, files, labels, classes, image_size=(256, 256), channels=3, batch_size=32):\n",
        "    self.files = files\n",
        "    self.labels = self.__to_categorical(labels, classes)\n",
        "    self.image_size = image_size\n",
        "    self.channels = channels\n",
        "    self.batch_size = batch_size\n",
        "    self.indexes = [i for i in range(len(files))]\n",
        "    np.random.shuffle(self.indexes)\n",
        "    # self.on_epoch_end()\n",
        "  \n",
        "  def __len__(self):\n",
        "    \"\"\"The number of batches per epoch\"\"\"\n",
        "    return int(np.floor(len(self.files) / self.batch_size))\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    \"\"\"Generate one batch of data\"\"\"\n",
        "    partition = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
        "    X, Y = self.__data_generation(partition)\n",
        "    return X, Y\n",
        "  \n",
        "  def on_epoch_end(self):\n",
        "    \"\"\"Shuffle files list and remap labels\"\"\"\n",
        "    np.random.shuffle(self.indexes)\n",
        "    \n",
        "  def __to_categorical(self, labels, classes):\n",
        "    \"\"\"Labels to numbers\"\"\"\n",
        "    classes_dict = {c:i for i,c in enumerate(classes)}\n",
        "    return [classes_dict[l] for l in labels]\n",
        "\n",
        "  def __data_generation(self, partition):\n",
        "    \"\"\"Load npy file and associate with a label\"\"\"\n",
        "    X = np.empty((self.batch_size, *self.image_size, self.channels))\n",
        "    Y = np.empty((self.batch_size,), dtype=int)\n",
        "\n",
        "    for i, index in enumerate(partition):\n",
        "      X[i,] = np.load(self.files[index])\n",
        "      Y[i] = self.labels[index]\n",
        "\n",
        "    return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ghBzSbF9fpw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess():\n",
        "  global train_generator, test_generator, val_generator\n",
        "  df = pd.read_csv('/content/stamps_scaled.csv')\n",
        "  E = df[df.CLASS == 'E']\n",
        "  S = df[df.CLASS == 'S']\n",
        "\n",
        "  train_df = pd.concat([E[0:3127], S[0:951]])\n",
        "  train_X = [f'/content/dataset/{f}.npy' for f in train_df.ID.tolist()]\n",
        "  train_Y = train_df.CLASS.tolist()\n",
        "\n",
        "  val_df = pd.concat([E[3128:5004], S[952:1522]])\n",
        "  val_X = [f'/content/dataset/{f}.npy' for f in val_df.ID.tolist()]\n",
        "  val_Y = val_df.CLASS.tolist()\n",
        "\n",
        "  test_df = pd.concat([E[5005:], S[1523:]])\n",
        "  test_X = [f'/content/dataset/{f}.npy' for f in test_df.ID.tolist()]\n",
        "  test_Y = test_df.CLASS.tolist()\n",
        "\n",
        "  train_generator = NpySequence(train_X, train_Y, ['E', 'S'])\n",
        "  val_generator = NpySequence(val_X, val_Y, ['E', 'S'])\n",
        "  test_generator = NpySequence(test_X, test_Y, ['E', 'S'])\n",
        "\n",
        "def compile_model(image_size):\n",
        "  global model\n",
        "  model = models.Sequential()\n",
        "  model.name = 'splus_convolutional'\n",
        "  model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(image_size + (3,))))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(512, activation='relu'))\n",
        "  model.add(layers.Dropout(.5))\n",
        "  model.add(layers.Dense(256, activation='relu'))\n",
        "  model.add(layers.Dropout(.5))\n",
        "  model.add(layers.Dense(256, activation='relu'))\n",
        "  model.add(layers.Dropout(.5))\n",
        "  model.add(layers.Dense(128, activation='relu'))\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(optimizer=optimizers.RMSprop(lr=5e-5),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['acc'])\n",
        "\n",
        "def fit(epochs=1):\n",
        "  global model, history, train_generator, val_generator\n",
        "  history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=80,\n",
        "      validation_steps=30,\n",
        "      epochs=epochs,\n",
        "      validation_data=val_generator,\n",
        "      callbacks=[fb.callbacks.FitsbookCallback()],\n",
        "      use_multiprocessing=True,\n",
        "      workers=4\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm3MY-kw29F0",
        "colab_type": "code",
        "outputId": "ef476649-6bc5-43de-82b5-f5fd28ca55f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "preprocess()\n",
        "compile_model((256, 256))\n",
        "fit(300)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Fitsbook]: Monitoring this training in real time https://natan.ninja/#/stats/32\n",
            "Epoch 1/300\n",
            "80/80 [==============================] - 148s 2s/step - loss: 0.5939 - acc: 0.7613 - val_loss: 0.5322 - val_acc: 0.7865\n",
            "Epoch 2/300\n",
            "46/80 [================>.............] - ETA: 46s - loss: 0.5567 - acc: 0.7629Epoch 1/300\n",
            "80/80 [==============================] - 136s 2s/step - loss: 0.5690 - acc: 0.7574 - val_loss: 0.5500 - val_acc: 0.7615\n",
            "Epoch 3/300\n",
            "79/80 [============================>.] - ETA: 1s - loss: 0.5494 - acc: 0.7674Epoch 1/300\n",
            "80/80 [==============================] - 153s 2s/step - loss: 0.5479 - acc: 0.7687 - val_loss: 0.5664 - val_acc: 0.7448\n",
            "Epoch 4/300\n",
            "80/80 [==============================] - 146s 2s/step - loss: 0.5380 - acc: 0.7742 - val_loss: 0.5351 - val_acc: 0.7708\n",
            "Epoch 5/300\n",
            "80/80 [==============================] - 139s 2s/step - loss: 0.5470 - acc: 0.7676 - val_loss: 0.5367 - val_acc: 0.7719\n",
            "Epoch 6/300\n",
            "80/80 [==============================] - 152s 2s/step - loss: 0.5519 - acc: 0.7586 - val_loss: 0.5488 - val_acc: 0.7583\n",
            "Epoch 7/300\n",
            "80/80 [==============================] - 143s 2s/step - loss: 0.5294 - acc: 0.7738 - val_loss: 0.5231 - val_acc: 0.7729\n",
            "Epoch 8/300\n",
            "80/80 [==============================] - 141s 2s/step - loss: 0.5411 - acc: 0.7594 - val_loss: 0.5290 - val_acc: 0.7615\n",
            "Epoch 9/300\n",
            "80/80 [==============================] - 151s 2s/step - loss: 0.5164 - acc: 0.7691 - val_loss: 0.5095 - val_acc: 0.7750\n",
            "Epoch 10/300\n",
            "80/80 [==============================] - 139s 2s/step - loss: 0.5227 - acc: 0.7633 - val_loss: 0.5134 - val_acc: 0.7656\n",
            "Epoch 11/300\n",
            "47/80 [================>.............] - ETA: 37s - loss: 0.5013 - acc: 0.7799"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}